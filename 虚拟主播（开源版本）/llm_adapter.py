"""llm_adapter.py
ç»Ÿä¸€çš„å¤§æ¨¡å‹æ¥å£é€‚é…å™¨ã€‚
æ”¯æŒ DeepSeekã€OpenAIã€Claudeã€Geminiã€æœ¬åœ°æ¨¡å‹ï¼Œå¹¶æŒ‰é…ç½®é¡ºåºä¾æ¬¡å°è¯•ã€‚
ç›®å‰ä»… DeepSeek ä¸ OpenAI æä¾›å®Œæ•´å®ç°ï¼Œå…¶ä½™è¿”å›æœªå®ç°å¼‚å¸¸ã€‚
"""
from __future__ import annotations

import logging
import os
import json
# For LocalProvider HTTP calls
import httpx
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class ProviderError(Exception):
    """ä¾› LLMProvider æŠ›å‡ºçš„ç»Ÿä¸€å¼‚å¸¸ï¼Œç”¨äºè§¦å‘å›é€€ã€‚"""


class BaseProvider:
    name: str = "base"

    def __init__(self, enabled: bool = True):
        self.enabled = enabled

    # pylint: disable=unused-argument
    def chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        model: str | None = None,
        max_tokens: int = 150,
        temperature: float = 0.8,
    ) -> str:
        raise ProviderError("not implemented")


class DeepSeekProvider(BaseProvider):
    name = "deepseek"

    def __init__(
        self,
        client,  # å·²åˆå§‹åŒ–çš„ deepseek.OpenAI å®ä¾‹
        default_model: str,
        enabled: bool = True,
    ):
        super().__init__(enabled)
        self._client = client
        self._default_model = default_model

    def chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        model: str | None = None,
        max_tokens: int = 150,
        temperature: float = 0.8,
    ) -> str:
        if not self.enabled:
            raise ProviderError("DeepSeekProvider disabled")
        response = self._client.chat.completions.create(
            model=model or self._default_model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()


class OpenAIProvider(BaseProvider):
    name = "openai"

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://api.openai.com/v1",
        default_model: str = "gpt-3.5-turbo",
        proxy: str | None = None,
        enabled: bool = True,
    ):
        super().__init__(enabled)
        try:
            import openai  # noqa: PLC0414
        except ImportError as exc:
            raise ProviderError("openai python-sdk not installed") from exc

        if not api_key:
            raise ProviderError("OPENAI_API_KEY missing")

        http_client = httpx.Client(proxies=proxy, timeout=60.0) if proxy else None
        if http_client:
            self._client = openai.OpenAI(api_key=api_key, base_url=base_url, http_client=http_client)
        else:
            self._client = openai.OpenAI(api_key=api_key, base_url=base_url)
        self._proxy = proxy
        self._default_model = default_model

    def chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        model: str | None = None,
        max_tokens: int = 150,
        temperature: float = 0.8,
    ) -> str:
        if not self.enabled:
            raise ProviderError("OpenAIProvider disabled")
        response = self._client.chat.completions.create(
            model=model or self._default_model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()


class GeminiProvider(BaseProvider):
    """Google Gemini (generative-ai) æä¾›è€…"""

    name = "gemini"

    def __init__(
        self,
        api_key: str,
        default_model: str = "gemini-pro",
        proxy: str | None = None,
        enabled: bool = True,
    ):
        super().__init__(enabled)

        try:
            import google.generativeai as genai  # type: ignore
        except ImportError as exc:
            raise ProviderError("google-generativeai æœªå®‰è£…ï¼Œè¯· pip install google-generativeai") from exc

        if not api_key:
            raise ProviderError("GEMINI_API_KEY missing")

        if proxy:
            os.environ["HTTPS_PROXY"] = proxy
            os.environ["HTTP_PROXY"] = proxy
        genai.configure(api_key=api_key)
        self._genai = genai
        self._default_model = default_model
        self._proxy = proxy

    def _merge_messages(self, messages: List[Dict[str, Any]]) -> str:
        """å°† OpenAI é£æ ¼ message åˆ—è¡¨åˆå¹¶ä¸ºçº¯æ–‡æœ¬ promptã€‚"""
        parts = []
        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", "")
            parts.append(f"{role}: {content}")
        return "\n".join(parts)

    def chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        model: str | None = None,
        max_tokens: int = 150,
        temperature: float = 0.8,
    ) -> str:
        if not self.enabled:
            raise ProviderError("GeminiProvider disabled")

        prompt_text = self._merge_messages(messages)

        mdl_name = model or self._default_model

        model_obj = self._genai.GenerativeModel(mdl_name)

        response = model_obj.generate_content(
            prompt_text,
            generation_config={
                "max_output_tokens": max_tokens,
                "temperature": temperature,
            },
        )

        return response.text.strip()


class ClaudeProvider(BaseProvider):
    """Anthropic Claude provider via official SDK."""

    name = "claude"

    def __init__(self, api_key: str, default_model: str = "claude-3-sonnet-20240229", proxy: str | None = None, enabled: bool = True):
        super().__init__(enabled)
        try:
            import anthropic  # type: ignore
        except ImportError as exc:
            raise ProviderError("anthropic SDK æœªå®‰è£…ï¼Œè¯· pip install anthropic") from exc

        if not api_key:
            raise ProviderError("ANTHROPIC_API_KEY missing")

        self._proxy = proxy
        if proxy:
            self._client = anthropic.Anthropic(api_key=api_key, proxies={"http": proxy, "https": proxy})
        else:
            self._client = anthropic.Anthropic(api_key=api_key)
        self._default_model = default_model

    def chat(self, messages: List[Dict[str, Any]], *, model=None, max_tokens=150, temperature=0.8):  # noqa: D401
        if not self.enabled:
            raise ProviderError("ClaudeProvider disabled")

        # Anthropic expects system, assistant, user list without names.
        claude_msgs = []
        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", "")
            claude_msgs.append({"role": role, "content": content})

        resp = self._client.messages.create(
            model=model or self._default_model,
            max_tokens=max_tokens,
            temperature=temperature,
            messages=claude_msgs,
        )
        return resp.content[-1].text.strip()  # type: ignore


class LocalProvider(BaseProvider):
    """è°ƒç”¨æœ¬åœ° OpenAI-compatible LLM ç«¯ç‚¹ï¼Œå¦‚ llama.cpp æˆ– ollama."""

    name = "local"

    def __init__(self, endpoint: str, default_model: str = "local-model", proxy: str | None = None, enabled: bool = True, timeout: float = 60.0):
        super().__init__(enabled)
        self._endpoint = endpoint.rstrip("/") + "/v1/chat/completions"
        self._default_model = default_model
        self._timeout = timeout
        self._proxy = proxy

    def chat(self, messages: List[Dict[str, Any]], *, model=None, max_tokens=150, temperature=0.8):  # noqa: D401
        if not self.enabled:
            raise ProviderError("LocalProvider disabled")

        payload = {
            "model": model or self._default_model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        try:
            r = httpx.post(self._endpoint, json=payload, timeout=self._timeout, proxies=self._proxy) if self._proxy else httpx.post(self._endpoint, json=payload, timeout=self._timeout)
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"].strip()
        except Exception as exc:  # noqa: BLE001
            raise ProviderError(f"LocalProvider HTTP error: {exc}") from exc


class DummyProvider(BaseProvider):
    """å ä½ç¬¦ï¼Œæš‚æœªå®ç°çš„æ¨¡å‹ã€‚"""

    def __init__(self, name: str):
        super().__init__(enabled=False)
        self.name = name

    def chat(self, *args, **kwargs) -> str:  # noqa: D401, ANN002
        raise ProviderError(f"Provider {self.name} not implemented")


class LLMRouter:
    """
    ç»Ÿä¸€çš„LLMè°ƒç”¨è·¯ç”±ã€‚
    æ ¹æ®é…ç½®åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æä¾›å•†ï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•è°ƒç”¨ã€‚
    """
    # å®šä¹‰äº†æ¨¡å‹çš„ä¼˜å…ˆçº§é¡ºåº
    PROVIDER_PRIORITY = ["gemini", "openai", "claude", "deepseek", "local"]

    def __init__(self, all_configs: Dict[str, str], deepseek_client_legacy=None):
        """
        ä»åŒ…å«æ‰€æœ‰é…ç½®çš„å­—å…¸åˆå§‹åŒ–è·¯ç”±å™¨ã€‚
        
        :param all_configs: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»æ•°æ®åº“è¯»å–çš„æ‰€æœ‰'key.name' -> 'value'é…ç½®ã€‚
        :param deepseek_client_legacy: (å…¼å®¹æ—§ç‰ˆ) é¢„åˆå§‹åŒ–çš„DeepSeekå®¢æˆ·ç«¯ã€‚
        """
        self.providers: List[BaseProvider] = []
        self._enabled_models: List[str] = []

        logger.info("ğŸ¤– åˆå§‹åŒ– LLM è·¯ç”±å™¨...")

        for provider_name in self.PROVIDER_PRIORITY:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ•°æ®åº“ä¸­è¢«å¯ç”¨
            is_enabled = all_configs.get(f"DEFAULT.{provider_name}.enable", "no").lower() == "yes"
            
            if not is_enabled:
                logger.debug(f"LLM æä¾›å•† '{provider_name}' æœªå¯ç”¨ï¼Œè·³è¿‡ã€‚")
                continue

            try:
                provider_instance = None
                logger.info(f"æ­£åœ¨é…ç½®å·²å¯ç”¨çš„ LLM æä¾›å•†: {provider_name}...")
                
                if provider_name == "deepseek":
                    # Deepseek çš„ client æ˜¯åœ¨ä¸»ç¨‹åºä¸­åˆ›å»ºçš„ï¼Œè¿™é‡Œæˆ‘ä»¬åªä½¿ç”¨å®ƒ
                    # ä½†æˆ‘ä»¬ä»ç„¶éœ€è¦æ£€æŸ¥å®ƒçš„é…ç½®æ˜¯å¦å®Œæ•´
                    api_key = all_configs.get("DEFAULT.deepseek.api_key")
                    if not api_key:
                       raise ProviderError("DeepSeek å·²å¯ç”¨ä½† API Key æœªé…ç½®ã€‚")
                    if deepseek_client_legacy:
                       provider_instance = DeepSeekProvider(
                           client=deepseek_client_legacy,
                           default_model=all_configs.get("DEFAULT.deepseek.model", "deepseek-chat"),
                           enabled=True
                       )
                    else: # å¦‚æœæ—§ç‰ˆ client ä¸å¯ç”¨ï¼Œåˆ™è‡ªå·±åˆ›å»ºä¸€ä¸ª
                        base_url = all_configs.get("DEFAULT.deepseek.api_base", "https://api.deepseek.com/v1")
                        proxy = all_configs.get("DEFAULT.deepseek.proxy") or all_configs.get("NETWORK.proxy")
                        http_client = httpx.Client(proxies=proxy, timeout=60.0) if proxy else None
                        import openai
                        client = openai.OpenAI(api_key=api_key, base_url=base_url, http_client=http_client)
                        provider_instance = DeepSeekProvider(client=client, default_model=all_configs.get("DEFAULT.deepseek.model", "deepseek-chat"), enabled=True)

                elif provider_name == "openai":
                    provider_instance = OpenAIProvider(
                        api_key=all_configs.get("DEFAULT.openai.api_key"),
                        base_url=all_configs.get("DEFAULT.openai.api_base", "https://api.openai.com/v1"),
                        default_model=all_configs.get("DEFAULT.openai.model", "gpt-4o"),
                        proxy=all_configs.get("DEFAULT.openai.proxy") or all_configs.get("NETWORK.proxy"),
                        enabled=True
                    )
                
                elif provider_name == "gemini":
                    provider_instance = GeminiProvider(
                        api_key=all_configs.get("DEFAULT.gemini.api_key"),
                        default_model=all_configs.get("DEFAULT.gemini.model", "gemini-1.5-pro-latest"),
                        proxy=all_configs.get("DEFAULT.gemini.proxy") or all_configs.get("NETWORK.proxy"),
                        enabled=True
                    )
                
                elif provider_name == "claude":
                    provider_instance = ClaudeProvider(
                        api_key=all_configs.get("DEFAULT.claude.api_key"),
                        default_model=all_configs.get("DEFAULT.claude.model", "claude-3-opus-20240229"),
                        proxy=all_configs.get("DEFAULT.claude.proxy") or all_configs.get("NETWORK.proxy"),
                        enabled=True
                    )

                elif provider_name == "local":
                    provider_instance = LocalProvider(
                        endpoint=all_configs.get("DEFAULT.local.endpoint"),
                        default_model=all_configs.get("DEFAULT.local.model", "local-model"),
                        proxy=all_configs.get("DEFAULT.local.proxy") or all_configs.get("NETWORK.proxy"),
                        enabled=True
                    )
                
                if provider_instance:
                    self.providers.append(provider_instance)
                    self._enabled_models.append(provider_name)
                    logger.info(f"âœ… LLM æä¾›å•† '{provider_name}' åŠ è½½æˆåŠŸã€‚")

            except ProviderError as e:
                logger.error(f"âŒ åŠ è½½ LLM æä¾›å•† '{provider_name}' å¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ LLM æä¾›å•† '{provider_name}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

        if not self.providers:
            logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½• LLM æä¾›å•†ï¼ŒAI å›å¤åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

    def get_enabled_models(self) -> List[str]:
        """è¿”å›å·²å¯ç”¨å¹¶æˆåŠŸåŠ è½½çš„æä¾›å•†åç§°åˆ—è¡¨ã€‚"""
        return self._enabled_models

    def chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        model: str | None = None, # model å‚æ•°ç°åœ¨ç”¨äºé€‰æ‹©ç‰¹å®šæ¨¡å‹ï¼Œå¦‚æœæœªæä¾›åˆ™æŒ‰é¡ºåºå°è¯•
        max_tokens: int = 150,
        temperature: float = 0.8,
    ) -> str:
        """
        æŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•è°ƒç”¨æä¾›å•†ï¼Œç›´åˆ°æˆåŠŸä¸ºæ­¢ã€‚
        å¦‚æœæŒ‡å®šäº† modelï¼Œåˆ™åªå°è¯•è¯¥æ¨¡å‹ã€‚
        """
        
        providers_to_try = self.providers
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œåˆ™åªä½¿ç”¨å¯¹åº”çš„æä¾›å•†
        if model:
            providers_to_try = [p for p in self.providers if p.name == model]
            if not providers_to_try:
                 raise ProviderError(f"æŒ‡å®šçš„æ¨¡å‹ '{model}' æœªå¯ç”¨æˆ–æœªæˆåŠŸåŠ è½½ã€‚")

        last_error: Exception | None = None
        for provider in providers_to_try:
            try:
                logger.info(f"â–¶ï¸ æ­£åœ¨å°è¯•ä½¿ç”¨ '{provider.name}' æ¨¡å‹...")
                return provider.chat(
                    messages=messages,
                    model=model, # ä¼ é€’ model å‚æ•°ç»™ provider
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
            except ProviderError as e:
                logger.warning(f"ğŸŸ¡ æ¨¡å‹ '{provider.name}' è°ƒç”¨å¤±è´¥ (ProviderError): {e}")
                last_error = e
            except Exception as e:
                logger.error(f"ğŸ”´ æ¨¡å‹ '{provider.name}' è°ƒç”¨æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
                last_error = e
        
        logger.error("âŒ æ‰€æœ‰å¯ç”¨çš„ LLM æä¾›å•†éƒ½è°ƒç”¨å¤±è´¥ã€‚")
        if last_error:
            raise last_error
        
        raise ProviderError("æ²¡æœ‰å¯ç”¨çš„ LLM æä¾›å•†ã€‚") 